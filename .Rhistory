#Stepwise regression w/ all interaction - HAS A COUPLE OF NA'S
set.seed(2928893)
mod.step.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.intr.RMSE <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"RMSE"]
mod.step.intr.r2 <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"Rsquared"]
#Model Selection - Comparing Models - Models with NA are removed
#Collecting the Errors
rMSPE <- as.matrix(cbind(mod.lm.RMSE, mod.lm.age.RMSE, mod.step.RMSE, mod.step.age.RMSE))
r2 <- as.matrix(cbind(mod.lm.r2, mod.lm.age.r2, mod.step.r2, mod.step.age.r2))
colnames(rMSPE) <- substr(colnames(rMSPE),1,nchar(colnames(rMSPE)) - 5)
colnames(r2) <- substr(colnames(r2),1,nchar(colnames(r2)) - 3)
mins.MSPE <- apply(rMSPE,1,min)
mins.r2 <- apply(r2,1,max)
#visualizations
boxplot(rMSPE, las = 2, main = "root-MSPE from 10-Fold CV")
boxplot(rMSPE/mins.MSPE, las = 2, main = "respective-root-MSPE from 10-Fold CV")
boxplot(r2, las = 2, main = "R2 value from 10-Fold CV")
##### Lm is the best model
# Hypothesis Testing - Do different ages have different slopes
anova(mod.lm$finalModel, mod.lm.age$finalModel)
# at alpha = 0.05 we fail to reject the null hypothesis
plot(mod.lm$finalModel)
plot(mod.lm.age$finalModel)
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#data Clensing pt2 -- Focused on Working out 2x a week by group and age
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!is.na(Age.years.), !is.na(LocationDesc), LocationDesc != "National") %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
#Introducing a new value
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("California"),Data_Value = 60, Age.years. = factor("18-24"))
data <- rbind(data,new_point)
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Workout Rate")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/Territory") + ylab("Workout Rate")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(Age.years.,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Workout Rate")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Work Out Rate", label = scales::comma)
# Model Selection- Train/Test Split (80/20)
test.size <- floor(0.8 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = test.size)
data.train <- data[train_ind,]
data.test <- data[-train_ind,]
data.train
# Model Selection - Building Models
#10 fold cross validation for comparision
control <- trainControl(method = "cv", number = 10,returnResamp="all")
#linear regression - no interaction
set.seed(2928893)
mod.lm <- train(Data_Value ~ ., method = "lm", trControl = control, data.train)
mod.lm.RMSE <- mod.lm$resample[,"RMSE"]
mod.lm.r2 <- mod.lm$resample[,"Rsquared"]
#Stepwise regression w/ BIC - no interaction
set.seed(2928893)
mod.step <- train(Data_Value ~., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.RMSE <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"RMSE"]
mod.step.r2 <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"Rsquared"]
## MODELS WITH 1 INTERACTION
#linear Regression - interaction on Age years
set.seed(2928893)
mod.lm.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.age.RMSE <- mod.lm.age$resample[,"RMSE"]
mod.lm.age.r2 <- mod.lm.age$resample[,"Rsquared"]
#linear Regression - interaction on LocationDesc HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "lm", trControl = control, data.train)
mod.lm.loc.RMSE <- mod.lm.loc$resample[,"RMSE"]
mod.lm.loc.r2 <- mod.lm.loc$resample[,"Rsquared"]
#Stepwise regression w/ BIC - interaction on Age years
set.seed(2928893)
mod.step.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.age.RMSE <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"RMSE"]
mod.step.age.r2 <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"Rsquared"]
#Stepwise regression w/ BIC - interaction on LocationDesc years HAS A COUPLE OF NA's
set.seed(2928893)
mod.step.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.loc.RMSE <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"RMSE"]
mod.step.loc.r2 <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"Rsquared"]
##MODELS WITH ALL INTERACTION TERMS
#linear Regression - all interaction HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.intr.RMSE <- mod.lm.intr$resample[,"RMSE"]
mod.lm.intr.r2 <- mod.lm.intr$resample[,"Rsquared"]
#Stepwise regression w/ all interaction - HAS A COUPLE OF NA'S
set.seed(2928893)
mod.step.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.intr.RMSE <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"RMSE"]
mod.step.intr.r2 <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"Rsquared"]
#Model Selection - Comparing Models - Models with NA are removed
#Collecting the Errors
rMSPE <- as.matrix(cbind(mod.lm.RMSE, mod.lm.age.RMSE, mod.step.RMSE, mod.step.age.RMSE))
r2 <- as.matrix(cbind(mod.lm.r2, mod.lm.age.r2, mod.step.r2, mod.step.age.r2))
colnames(rMSPE) <- substr(colnames(rMSPE),1,nchar(colnames(rMSPE)) - 5)
colnames(r2) <- substr(colnames(r2),1,nchar(colnames(r2)) - 3)
mins.MSPE <- apply(rMSPE,1,min)
mins.r2 <- apply(r2,1,max)
#visualizations
boxplot(rMSPE, las = 2, main = "root-MSPE from 10-Fold CV")
boxplot(rMSPE/mins.MSPE, las = 2, main = "respective-root-MSPE from 10-Fold CV")
boxplot(r2, las = 2, main = "R2 value from 10-Fold CV")
##### Lm.age is the best model
# Hypothesis Testing - Do different ages have different slopes
anova(mod.lm$finalModel, mod.lm.age$finalModel)
# at alpha = 0.05 we reject the null hypothesis
#Residual Analysis
plot(mod.lm.age$finalModel)
mod.lm.intr$resample
plot(mod.lm$finalModel)
#4-heteroscedasticity is true since median is centered around 0 and distribution around it is pretty even
summary(mod.lm$finalModel)
# reject null since p-value is smaller than 0.05 significance
vif(mod.lm$finalModel)
#VIF values are all below 5 so we know there is no multicolinarity problems in the dataset
plot(mod.lm$finalModel$residuals)
#confirms constant varience, is in agreement with summary data with median being around 0 and data distributed
#equally
# Predicting Y in test dataset(For final model)
mod.lm.pred <- predict(mod.lm, newdata = data.test)
# Priting top 6 rows of actual and predited Y values(just for final model)
pred.mod.lm <- cbind(data.test$Data_Value, mod.lm.pred,)
head(pred.mod.lm)
#As we can see there are some outliers but we cant take them out since it would affect the model.
mod.lm$cooksd <- cooks.distance(mod.lm$finalModel)
mod.lm$cooksd <- ifelse(mod.lm$cooksd < 4/nrow(data), "Not Outlier","Outlier")
mod.lm$cooksd
#loading libraries
library(tidyverse)
library(caret)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#loading libraries
library(tidyverse)
library(caret)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#data Clensing pt2 -- Focused on Working out 2x a week by group and age
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!is.na(Age.years.), !is.na(LocationDesc), LocationDesc != "National") %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
#Introducing a new value
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("California"),Data_Value = 60, Age.years. = factor("18-24"))
data <- rbind(data,new_point)
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Workout Rate")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/Territory") + ylab("Workout Rate")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(Age.years.,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Workout Rate")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Work Out Rate", label = scales::comma)
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#data Clensing pt2 -- Focused on Working out 2x a week by group and age
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!is.na(Age.years.), !is.na(LocationDesc), LocationDesc != "National") %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
#Introducing a new value
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("California"),Data_Value = 60, Age.years. = factor("18-24"))
data <- rbind(data,new_point)
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Workout Rate")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/Territory") + ylab("Workout Rate")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(Age.years.,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Workout Rate")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Work Out Rate", label = scales::comma)
# Model Selection- Train/Test Split (80/20)
test.size <- floor(0.8 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = test.size)
data.train <- data[train_ind,]
data.test <- data[-train_ind,]
data.train
# Model Selection - Building Models
#10 fold cross validation for comparision
control <- trainControl(method = "cv", number = 10,returnResamp="all")
#linear regression - no interaction
set.seed(2928893)
mod.lm <- train(Data_Value ~ ., method = "lm", trControl = control, data.train)
mod.lm.RMSE <- mod.lm$resample[,"RMSE"]
mod.lm.r2 <- mod.lm$resample[,"Rsquared"]
#Stepwise regression w/ BIC - no interaction
set.seed(2928893)
mod.step <- train(Data_Value ~., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.RMSE <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"RMSE"]
mod.step.r2 <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"Rsquared"]
## MODELS WITH 1 INTERACTION
#linear Regression - interaction on Age years
set.seed(2928893)
mod.lm.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.age.RMSE <- mod.lm.age$resample[,"RMSE"]
mod.lm.age.r2 <- mod.lm.age$resample[,"Rsquared"]
#linear Regression - interaction on LocationDesc HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "lm", trControl = control, data.train)
mod.lm.loc.RMSE <- mod.lm.loc$resample[,"RMSE"]
mod.lm.loc.r2 <- mod.lm.loc$resample[,"Rsquared"]
#Stepwise regression w/ BIC - interaction on Age years
set.seed(2928893)
mod.step.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.age.RMSE <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"RMSE"]
mod.step.age.r2 <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"Rsquared"]
#Stepwise regression w/ BIC - interaction on LocationDesc years HAS A COUPLE OF NA's
set.seed(2928893)
mod.step.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.loc.RMSE <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"RMSE"]
mod.step.loc.r2 <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"Rsquared"]
##MODELS WITH ALL INTERACTION TERMS
#linear Regression - all interaction HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.intr.RMSE <- mod.lm.intr$resample[,"RMSE"]
mod.lm.intr.r2 <- mod.lm.intr$resample[,"Rsquared"]
#Stepwise regression w/ all interaction - HAS A COUPLE OF NA'S
set.seed(2928893)
mod.step.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.intr.RMSE <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"RMSE"]
mod.step.intr.r2 <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"Rsquared"]
#Model Selection - Comparing Models - Models with NA are removed
#Collecting the Errors
rMSPE <- as.matrix(cbind(mod.lm.RMSE, mod.lm.age.RMSE, mod.step.RMSE, mod.step.age.RMSE))
r2 <- as.matrix(cbind(mod.lm.r2, mod.lm.age.r2, mod.step.r2, mod.step.age.r2))
colnames(rMSPE) <- substr(colnames(rMSPE),1,nchar(colnames(rMSPE)) - 5)
colnames(r2) <- substr(colnames(r2),1,nchar(colnames(r2)) - 3)
mins.MSPE <- apply(rMSPE,1,min)
mins.r2 <- apply(r2,1,max)
#visualizations
boxplot(rMSPE, las = 2, main = "root-MSPE from 10-Fold CV")
boxplot(rMSPE/mins.MSPE, las = 2, main = "respective-root-MSPE from 10-Fold CV")
boxplot(r2, las = 2, main = "R2 value from 10-Fold CV")
##### Lm.age is the best model
# Hypothesis Testing - Do different ages have different slopes
anova(mod.lm$finalModel, mod.lm.age$finalModel)
# at alpha = 0.05 we reject the null hypothesis
#Residual Analysis
plot(mod.lm.age$finalModel)
#Residual Analysis
plot(mod.lm.age$finalModel)
## RESULTS
mod.lm.pred <- predict(mod.lm, newdata = data.test)
mod.lm.age.pred <- predict(mod.lm.age, newdata = data.test)
mod.step.pred <- predict(mod.step, newdata = data.test)
mod.step.age.pred <- predict(mod.step.age, newdata = data.test)
pred <- cbind(mod.lm.pred, mod.lm.age.pred,  mod.step.pred, mod.step.age.pred)
calculateMSPE <- function(mod,test){
result <- matrix(nrow = 1, ncol = ncol(mod))
colnames(result) <- colnames(mod)
for(i in 1:ncol(mod)){
result[i] <- mean((test$Data_Value - mod[,i])^2)
}
result
}
MSPE.test <- calculateMSPE(pred,data.test)
MSPE.test
# MODEL WITH AGE INTERACTION WITH STATE PERFORMS THE BEST
#Testing with an abline
plot(data.test$Data_Value, mod.lm.age.pred)
abline(c(0,1))
plot(data.test$Data_Value, mod.lm.pred)
abline(c(0,1))
plot(mod.lm.age$finalModel)
#4-heteroscedasticity is true since median is centered around 0 and distribution around it is pretty even
summary(mod.lm$finalModel)
# reject null since p-value is smaller than 0.05 significance
summary(mod.lm.age$finalModel)
# reject null since p-value is smaller than 0.05 significance
vif(mod.lm$finalModel)
#VIF values are all below 5 so we know there is no multicolinarity problems in the dataset
plot(mod.lm.age$finalModel$residuals)
#confirms constant varience, is in agreement with summary data with median being around 0 and data distributed
#equally
# Predicting Y in test dataset(For final model)
mod.lm.pred <- predict(mod.lm, newdata = data.test)
# Priting top 6 rows of actual and predited Y values(just for final model)
pred.mod.lm <- cbind(data.test$Data_Value, mod.lm.pred,)
head(pred.mod.lm)
#As we can see there are some outliers but we cant take them out since it would affect the model.
mod.lm$cooksd <- cooks.distance(mod.lm$finalModel)
mod.lm$cooksd <- ifelse(mod.lm$cooksd < 4/nrow(data), "Not Outlier","Outlier")
mod.lm$cooksd
library(MASS)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
data
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
