#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#data Clensing pt2 -- Focused on Working out 2x a week by group and age
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!is.na(Age.years.), !is.na(LocationDesc), LocationDesc != "National") %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
#Introducing a new value
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("California"),Data_Value = 60, Age.years. = factor("18-24"))
data <- rbind(data,new_point)
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Workout Rate")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/Territory") + ylab("Workout Rate")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(Age.years.,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Workout Rate")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Work Out Rate", label = scales::comma)
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
data <- droplevels(data)
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
data <- droplevels(data)
head(data)
#data Clensing pt2 -- Focused on Working out 2x a week by group and age
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!is.na(Age.years.), !is.na(LocationDesc), LocationDesc != "National") %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
#Introducing a new value
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("California"),Data_Value = 60, Age.years. = factor("18-24"))
data <- rbind(data,new_point)
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Workout Rate")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/Territory") + ylab("Workout Rate")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(Age.years.,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Workout Rate")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Work Out Rate", label = scales::comma)
# Model Selection- Train/Test Split (80/20)
test.size <- floor(0.8 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = test.size)
data.train <- data[train_ind,]
data.test <- data[-train_ind,]
data.train
# Model Selection - Building Models
#10 fold cross validation for comparision
control <- trainControl(method = "cv", number = 10,returnResamp="all")
#linear regression - no interaction
set.seed(2928893)
mod.lm <- train(Data_Value ~ ., method = "lm", trControl = control, data.train)
mod.lm.RMSE <- mod.lm$resample[,"RMSE"]
mod.lm.r2 <- mod.lm$resample[,"Rsquared"]
#Stepwise regression w/ BIC - no interaction
set.seed(2928893)
mod.step <- train(Data_Value ~., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.RMSE <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"RMSE"]
mod.step.r2 <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"Rsquared"]
## MODELS WITH 1 INTERACTION
#linear Regression - interaction on Age years
set.seed(2928893)
mod.lm.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.age.RMSE <- mod.lm.age$resample[,"RMSE"]
mod.lm.age.r2 <- mod.lm.age$resample[,"Rsquared"]
#linear Regression - interaction on LocationDesc HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "lm", trControl = control, data.train)
mod.lm.loc.RMSE <- mod.lm.loc$resample[,"RMSE"]
mod.lm.loc.r2 <- mod.lm.loc$resample[,"Rsquared"]
#Stepwise regression w/ BIC - interaction on Age years
set.seed(2928893)
mod.step.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.age.RMSE <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"RMSE"]
mod.step.age.r2 <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"Rsquared"]
#Stepwise regression w/ BIC - interaction on LocationDesc years HAS A COUPLE OF NA's
set.seed(2928893)
mod.step.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.loc.RMSE <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"RMSE"]
mod.step.loc.r2 <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"Rsquared"]
##MODELS WITH ALL INTERACTION TERMS
#linear Regression - all interaction HAS A COUPLE OF NA'S
set.seed(2928893)
mod.lm.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.intr.RMSE <- mod.lm.intr$resample[,"RMSE"]
mod.lm.intr.r2 <- mod.lm.intr$resample[,"Rsquared"]
#Stepwise regression w/ all interaction - HAS A COUPLE OF NA'S
set.seed(2928893)
mod.step.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.intr.RMSE <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"RMSE"]
mod.step.intr.r2 <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"Rsquared"]
#Model Selection - Comparing Models - Models with NA are removed
#Collecting the Errors
rMSPE <- as.matrix(cbind(mod.lm.RMSE, mod.lm.age.RMSE, mod.step.RMSE, mod.step.age.RMSE))
r2 <- as.matrix(cbind(mod.lm.r2, mod.lm.age.r2, mod.step.r2, mod.step.age.r2))
colnames(rMSPE) <- substr(colnames(rMSPE),1,nchar(colnames(rMSPE)) - 5)
colnames(r2) <- substr(colnames(r2),1,nchar(colnames(r2)) - 3)
mins.MSPE <- apply(rMSPE,1,min)
mins.r2 <- apply(r2,1,max)
#visualizations
boxplot(rMSPE, las = 2, main = "root-MSPE from 10-Fold CV")
boxplot(rMSPE/mins.MSPE, las = 2, main = "respective-root-MSPE from 10-Fold CV")
boxplot(r2, las = 2, main = "R2 value from 10-Fold CV")
##### Lm.age is the best model
# Hypothesis Testing - Do different ages have different slopes
anova(mod.lm$finalModel, mod.lm.age$finalModel)
# at alpha = 0.05 we reject the null hypothesis
#Residual Analysis
plot(mod.lm.age$finalModel)
#Residual Analysis
plot(mod.lm.age$finalModel)
## RESULTS
mod.lm.pred <- predict(mod.lm, newdata = data.test)
mod.lm.age.pred <- predict(mod.lm.age, newdata = data.test)
mod.step.pred <- predict(mod.step, newdata = data.test)
mod.step.age.pred <- predict(mod.step.age, newdata = data.test)
pred <- cbind(mod.lm.pred, mod.lm.age.pred,  mod.step.pred, mod.step.age.pred)
calculateMSPE <- function(mod,test){
result <- matrix(nrow = 1, ncol = ncol(mod))
colnames(result) <- colnames(mod)
for(i in 1:ncol(mod)){
result[i] <- mean((test$Data_Value - mod[,i])^2)
}
result
}
MSPE.test <- calculateMSPE(pred,data.test)
MSPE.test
# MODEL WITH AGE INTERACTION WITH STATE PERFORMS THE BEST
#Testing with an abline
plot(data.test$Data_Value, mod.lm.age.pred)
abline(c(0,1))
plot(data.test$Data_Value, mod.lm.pred)
abline(c(0,1))
plot(mod.lm.age$finalModel)
#4-heteroscedasticity is true since median is centered around 0 and distribution around it is pretty even
summary(mod.lm$finalModel)
# reject null since p-value is smaller than 0.05 significance
summary(mod.lm.age$finalModel)
# reject null since p-value is smaller than 0.05 significance
vif(mod.lm$finalModel)
#VIF values are all below 5 so we know there is no multicolinarity problems in the dataset
plot(mod.lm.age$finalModel$residuals)
#confirms constant varience, is in agreement with summary data with median being around 0 and data distributed
#equally
# Predicting Y in test dataset(For final model)
mod.lm.pred <- predict(mod.lm, newdata = data.test)
# Priting top 6 rows of actual and predited Y values(just for final model)
pred.mod.lm <- cbind(data.test$Data_Value, mod.lm.pred,)
head(pred.mod.lm)
#As we can see there are some outliers but we cant take them out since it would affect the model.
mod.lm$cooksd <- cooks.distance(mod.lm$finalModel)
mod.lm$cooksd <- ifelse(mod.lm$cooksd < 4/nrow(data), "Not Outlier","Outlier")
mod.lm$cooksd
library(MASS)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt, Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(MASS)
library(leaps)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
data
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
#Removing Redundant Columns
summary(data)
knitr::opts_chunk$set(echo = TRUE)
#Check for influential points
cooksd <- cooks.distance(mod.lm$finalModel)
knitr::opts_chunk$set(echo = TRUE)
#loading libraries
library(tidyverse)
library(caret)
library(faraway)
library(usmap)
#This section of code loads in the data, deletes columns with extraeous information, and filters the data to Obesity/Weight Status
#loading data
data <- read.csv("nutrition_data.csv", header = TRUE,na.strings=c("","NA"))
min(data$YearEnd)
#Removing Redundant Columns
summary(data)
# Delete Location Desc - LocationAbbr is the same column
# Delete Datasource - All Data comes from the same location
# Delete Topic - Same information As Class
# Delete Data_value_Unit & Data_Value_Type - Unit is all NA and All the data is a value of a type
# Delete Data_Value_Alt - Same information as Data_Value
# Delete Data_Value_FootNote_Symbol & Data_Value_Footnote - Provides information about missing samples
# Delete Total - Adds in no information
# Delete ClassID, TopicID, QUestionID, DataValueTypeID, LocationID, StratificationCategoryID1, StratificationID1
# Delete Low_Confidence_Limit, High_Confidence_Limit, Sample Size - calulated confidence intervals for each sample size
#Other Notes
# Both Confidence Limits, Sample Size
# Age, Education, Gender, Income, Race has a lot of missing Values
# Filter Class to Obesity/Weight Status - Physical Activity and Fruits and Vegetables are irrelevent to the data
# Question TBA
#deleting redundant values
data <-  data %>% select(-c(LocationAbbr,Datasource, Topic,Data_Value_Unit,Data_Value_Type,Data_Value_Alt,Data_Value_Footnote,Data_Value_Footnote_Symbol, Total, ClassID, TopicID, Data_Value_Unit, QuestionID, DataValueTypeID, LocationID, StratificationCategory1, StratificationCategoryId1,Stratification1, StratificationID1))
#Filtering the Data to focus on A SPECIFIC QUESTION, OVERWEIGHT IS REMOVED
summary(data$Class)
data <- data %>% filter(Class == "Physical Activity" & Question == "Percent of adults who engage in muscle-strengthening activities on 2 or more days a week") %>% select(-c(Question, Class))
#Checking for length of study (YearEnd - YearStart) to see if there's studies crossing multiple years
data %>% filter( YearEnd - YearStart > 0)
data <- data %>% select(-YearEnd)
#Removes Confidence Interval + Sample Size + Geolocation
data <- data %>% select(-c("Low_Confidence_Limit","High_Confidence_Limit","Sample_Size", "GeoLocation"))
#filtering to view age + filtering null GeoLocation Values
data <- data %>% filter(!as.character(LocationDesc) %in% c("National","Guam","Puerto Rico"), !is.na(Age.years.)) %>% select(-c(Education, Gender, Income, Race.Ethnicity))
#checks if there's any more missing values
summary(!is.na(data))
data <- droplevels(data)
head(data)
#Introducing a new value
data %>% filter(YearStart == 2011, LocationDesc == "Alabama",Age.years. == "18 - 24")
new_point <- data.frame(YearStart = 2011,LocationDesc = factor("Alabama"),Data_Value = 60, Age.years. = factor("18 - 24"))
data <- rbind(data,new_point)
#Data Visualization - Distribution
data %>% ggplot(aes(x = YearStart)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Count") + ggtitle("Distribution of Data Collected Each Year")
data %>% ggplot(aes(x = LocationDesc)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/District") + ylab("Count") + ggtitle("Distribution of Surveys Conducted in Each State/District")
data %>% ggplot(aes(x = Age.years.)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Count") + ggtitle("Distribution of Age Groupings")
data %>% ggplot(aes(x = Data_Value)) + geom_density()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Proportion of Adults That Satisfies CDC Recommendations For Muscle Strengthening Activities") + ylab("Density") + ggtitle("Density of Adults That Satisfies CDC Recommendations")
#Data Visualization
# Year vs Work Out Rate
data %>% group_by(YearStart) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(YearStart,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Year") + ylab("Proportion") + ggtitle("Proportion of Adults That Satisfy CDC Guidelines vs Year")
# Age Group vs Work out Rate
data %>% group_by(Age.years.) %>%  ggplot(aes(x = Age.years., y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("Age Grouping") + ylab("Proportion") + ggtitle("Proportion of Adults That Satisfy CDC Guidelines vs Age Groupings")
# State vs Work Out Rate
data %>% group_by(LocationDesc) %>% mutate(median_value = median(Data_Value)) %>% ggplot(aes(x = reorder(LocationDesc,median_value), y = Data_Value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + xlab("US State/District") + ylab("Proportion") + ggtitle("Proportion of Adults That Satisfy CDC Guidelines vs US State/District")
# Heatmap of states
data_temp <- data %>% rename("state" = "LocationDesc")
plot_usmap(data = data_temp, values = "Data_Value", color = "red", labels = TRUE) + scale_fill_continuous(name = "Proportion", label = scales::comma) + ggtitle("Heatmap of Adults that Satisfy CDC Guidelines for Muscle Strengthening Exercise")
#pairs plot for correlation
pairs(data)
# Train/Test Split (80/20)
set.seed(2928893)
test.size <- floor(0.8 * nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = test.size)
data.train <- data[train_ind,]
data.test <- data[-train_ind,]
row.names(data.train) <- NULL
rownames(data.test) <- NULL
# Hypothesis testing - Model Building
#linear regression - no interaction
train.lm <- lm(Data_Value ~., data.train)
train.lm.noyears <- lm(Data_Value ~ LocationDesc + Age.years., data.train)
#interaction Model on Age
train.lm.age <- lm(Data_Value ~  YearStart*Age.years. + LocationDesc, data.train)
#interaction Model on Location
train.lm.loc <- lm(Data_Value ~  YearStart*LocationDesc + Age.years., data.train)
#interaction model on all
train.lm.all <- lm(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, data.train)
#Hypothesis Testing - Importance of Age.years
anova(train.lm, train.lm.noyears)
#Hypothesis Testing - Interaction on age
anova(train.lm,train.lm.age)
#Hypothesis Testing - Interaction on location
anova(train.lm, train.lm.loc)
#Hypothesis Testing - Interaction on All
anova(train.lm.age, train.lm.all)
# Model Selection - Building Models
#10 fold cross validation for comparision
control <- trainControl(method = "cv", number = 10,returnResamp="all")
#linear regression - no interaction
set.seed(2928893)
mod.lm <- train(Data_Value ~ ., method = "lm", trControl = control, data.train)
mod.lm.RMSE <- mod.lm$resample[,"RMSE"]
mod.lm.r2 <- mod.lm$resample[,"Rsquared"]
#Stepwise regression w/ BIC - no interaction
set.seed(2928893)
mod.step <- train(Data_Value ~., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.RMSE <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"RMSE"]
mod.step.r2 <- mod.step$resample[mod.step$resample[,"nvmax"] == mod.step$bestTune[1,1],"Rsquared"]
## MODELS WITH 1 INTERACTION
#linear Regression - interaction on Age years
set.seed(2928893)
mod.lm.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.age.RMSE <- mod.lm.age$resample[,"RMSE"]
mod.lm.age.r2 <- mod.lm.age$resample[,"Rsquared"]
#linear Regression - interaction on LocationDesc
set.seed(2928893)
mod.lm.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "lm", trControl = control, data.train)
mod.lm.loc.RMSE <- mod.lm.loc$resample[,"RMSE"]
mod.lm.loc.r2 <- mod.lm.loc$resample[,"Rsquared"]
#Stepwise regression w/ BIC - interaction on Age years
set.seed(2928893)
mod.step.age <- train(Data_Value ~  YearStart*Age.years. + LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.age.RMSE <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"RMSE"]
mod.step.age.r2 <- mod.step.age$resample[mod.step.age$resample[,"nvmax"] == mod.step.age$bestTune[1,1],"Rsquared"]
#Stepwise regression w/ BIC - interaction on LocationDesc
set.seed(2928893)
mod.step.loc <- train(Data_Value ~  YearStart*LocationDesc + Age.years., method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.loc.RMSE <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"RMSE"]
mod.step.loc.r2 <- mod.step.loc$resample[mod.step.loc$resample[,"nvmax"] == mod.step.loc$bestTune[1,1],"Rsquared"]
##MODELS WITH ALL INTERACTION TERMS
#linear Regression - all interaction
set.seed(2928893)
mod.lm.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "lm", trControl = control, data.train)
mod.lm.intr.RMSE <- mod.lm.intr$resample[,"RMSE"]
mod.lm.intr.r2 <- mod.lm.intr$resample[,"Rsquared"]
#Stepwise regression w/ all interaction
set.seed(2928893)
mod.step.intr <- train(Data_Value ~  YearStart + Age.years. + LocationDesc + YearStart:Age.years. + YearStart:LocationDesc, method = "leapSeq", trControl = control, data.train, trace = FALSE, k = log(nrow(data.train)))
mod.step.intr.RMSE <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"RMSE"]
mod.step.intr.r2 <- mod.step.intr$resample[mod.step.intr$resample[,"nvmax"] == mod.step.intr$bestTune[1,1],"Rsquared"]
#Model Selection - Comparing Models
#Collecting the Errors
rMSPE <- as.matrix(cbind(mod.lm.RMSE, mod.lm.age.RMSE,mod.lm.loc.RMSE,mod.lm.intr.RMSE, mod.step.RMSE, mod.step.age.RMSE,mod.step.loc.RMSE,mod.step.intr.RMSE))
r2 <- as.matrix(cbind(mod.lm.r2, mod.lm.age.r2,mod.lm.loc.r2,mod.lm.intr.r2, mod.step.r2, mod.step.age.r2,mod.step.loc.r2,mod.step.intr.r2))
colnames(rMSPE) <- substr(colnames(rMSPE),1,nchar(colnames(rMSPE)) - 5)
colnames(r2) <- substr(colnames(r2),1,nchar(colnames(r2)) - 3)
mins.MSPE <- apply(rMSPE,1,min)
mins.r2 <- apply(r2,1,max)
#visualizations
boxplot(rMSPE, las = 2, main = "root-MSPE from 10-Fold CV", xlab = "model", ylab = "root-MSPE")
boxplot(rMSPE/mins.MSPE, las = 2, main = "Relative-root-MSPE from 10-Fold CV",xlab = "model", ylab = "Relative root-MSPE")
boxplot(r2, las = 2, main = "R2 value from 10-Fold CV", xlab = "model", ylab = "R2")
## All of the LM models perform decently well
## RESULTS
mod.lm.pred <- predict(mod.lm, newdata = data.test)
mod.lm.age.pred <- predict(mod.lm.age, newdata = data.test)
mod.lm.loc.pred <- predict(mod.lm.loc, newdata = data.test)
mod.lm.intr.pred <- predict(mod.lm.intr, newdata = data.test)
mod.step.pred <- predict(mod.step, newdata = data.test)
mod.step.age.pred <- predict(mod.step.age, newdata = data.test)
mod.step.loc.pred <- predict(mod.step.loc, newdata = data.test)
mod.step.intr.pred <- predict(mod.step.intr, newdata = data.test)
pred <- cbind(mod.lm.pred, mod.lm.age.pred,mod.lm.loc.pred
,mod.lm.intr.pred, mod.step.pred, mod.step.age.pred,mod.step.loc.pred
,mod.step.intr.pred)
calculateMSPE <- function(mod,test){
result <- matrix(nrow = 1, ncol = ncol(mod))
colnames(result) <- colnames(mod)
for(i in 1:ncol(mod)){
result[i] <- mean((test$Data_Value - mod[,i])^2)
}
result
}
calculateR2 <- function(mod,test){
result <- matrix(nrow = 1, ncol = ncol(mod))
colnames(result) <- colnames(mod)
for(i in 1:ncol(mod)){
result[i] <- cor(mod[,i],test$Data_Value)^2
}
result
}
MSPE.test <- calculateMSPE(pred,data.test)
MSPE.test
# MODEL WITH AGE INTERACTION WITH STATE PERFORMS THE BEST
r2.test <- calculateR2(pred, data.test)
r2.test
#Testing with an abline
plot(data.test$Data_Value, mod.lm.intr.pred)
abline(c(0,1))
summary(mod.lm$finalModel)
#Residual Analysis
plot(mod.lm.intr$finalModel)
#Checking assumptions of linear Regression
#1-Independence of observations is assumed
#2-Linearity is satisfied from the given q-q plot
#3-Normality is confirmed due to distribution of the residual in Histogram
#4-heteroscedasticity is true since median is centered around 0 and distribution around it is pretty even
vif(mod.lm$finalModel)
#VIF values are all below 5 so we know there is no multicolinarity problems in the dataset
#Check for influential points
cooksd <- cooks.distance(mod.lm$finalModel)
which(cooksd > 1)
#leverage points
#High H_ii values
H <- diag(lm.influence(mod.lm$finalModel)$hat)
2*p/nrow(data.train)
#leverage points
#High H_ii values
H <- diag(lm.influence(mod.lm$finalModel)$hat)
p <- length(mod.lm$finalModel$coefficients)
2*p/nrow(data.train)
which(H > 2*p/nrow(data.train), arr.ind = TRUE)
#Large Residuals
student.resid <- rstudent(mod.lm$finalModel)
which(abs(student.resid) >= 3)
##The points 160 has a high H_ii and has a large residual
#data.train[160,]
which(H > 2*p/nrow(data.train), arr.ind = TRUE)
